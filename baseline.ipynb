{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Source","metadata":{}},{"cell_type":"markdown","source":"* Paper source: Automatic Pull Request Title Generation\n* Dataset: https://github.com/soarsmu/PRTiger/raw/main/data/PRTiger.zip\n* model source: https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb","metadata":{}},{"cell_type":"markdown","source":"# Load dataset","metadata":{"id":"4qQjXtKDLZ_o"}},{"cell_type":"code","source":"PATH = '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2023-05-22T14:04:52.211569Z","iopub.execute_input":"2023-05-22T14:04:52.212018Z","iopub.status.idle":"2023-05-22T14:04:52.225456Z","shell.execute_reply.started":"2023-05-22T14:04:52.211986Z","shell.execute_reply":"2023-05-22T14:04:52.224551Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/soarsmu/PRTiger/raw/main/data/PRTiger.zip","metadata":{"id":"WWoB-D4pLcGK","outputId":"0edf5791-0220-41b2-8ce9-312e9f8d39a6","execution":{"iopub.status.busy":"2023-05-22T14:04:52.227149Z","iopub.execute_input":"2023-05-22T14:04:52.227426Z","iopub.status.idle":"2023-05-22T14:04:55.107614Z","shell.execute_reply.started":"2023-05-22T14:04:52.227404Z","shell.execute_reply":"2023-05-22T14:04:55.106370Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2023-05-22 14:04:53--  https://github.com/soarsmu/PRTiger/raw/main/data/PRTiger.zip\nResolving github.com (github.com)... 192.30.255.113\nConnecting to github.com (github.com)|192.30.255.113|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/soarsmu/PRTiger/main/data/PRTiger.zip [following]\n--2023-05-22 14:04:53--  https://raw.githubusercontent.com/soarsmu/PRTiger/main/data/PRTiger.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 25051895 (24M) [application/zip]\nSaving to: ‘PRTiger.zip’\n\nPRTiger.zip         100%[===================>]  23.89M  --.-KB/s    in 0.1s    \n\n2023-05-22 14:04:54 (188 MB/s) - ‘PRTiger.zip’ saved [25051895/25051895]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!unzip -a {PATH}/PRTiger.zip","metadata":{"id":"kbGNpADHLd6d","outputId":"5ff8ac4d-8317-470b-93c8-328a85518957","execution":{"iopub.status.busy":"2023-05-22T14:04:55.112579Z","iopub.execute_input":"2023-05-22T14:04:55.112952Z","iopub.status.idle":"2023-05-22T14:04:57.126992Z","shell.execute_reply.started":"2023-05-22T14:04:55.112915Z","shell.execute_reply":"2023-05-22T14:04:57.125606Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Archive:  /kaggle/working/PRTiger.zip\n   creating: PRTiger/\n   creating: PRTiger/no-token/\n  inflating: PRTiger/.DS_Store       [binary]\n  inflating: __MACOSX/PRTiger/._.DS_Store  [binary]\n   creating: PRTiger/with-token/\n  inflating: PRTiger/no-token/valid.csv  [binary]\n  inflating: __MACOSX/PRTiger/no-token/._valid.csv  [binary]\n  inflating: PRTiger/no-token/test.csv  [binary]\n  inflating: __MACOSX/PRTiger/no-token/._test.csv  [binary]\n  inflating: PRTiger/no-token/train.csv  [binary]\n  inflating: __MACOSX/PRTiger/no-token/._train.csv  [binary]\n  inflating: PRTiger/with-token/valid.csv  [binary]\n  inflating: __MACOSX/PRTiger/with-token/._valid.csv  [binary]\n  inflating: PRTiger/with-token/test.csv  [binary]\n  inflating: __MACOSX/PRTiger/with-token/._test.csv  [binary]\n  inflating: PRTiger/with-token/train.csv  [binary]\n  inflating: __MACOSX/PRTiger/with-token/._train.csv  [binary]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"! pip install transformers\n! pip install datasets\n! pip install sentencepiece\n! pip install rouge_score","metadata":{"id":"_gaaojSBoQ5f","outputId":"21c5341b-b9ea-4bb6-8d84-c0a875387bee","execution":{"iopub.status.busy":"2023-05-22T14:04:57.130776Z","iopub.execute_input":"2023-05-22T14:04:57.132579Z","iopub.status.idle":"2023-05-22T14:05:45.822871Z","shell.execute_reply.started":"2023-05-22T14:04:57.132533Z","shell.execute_reply":"2023-05-22T14:05:45.821531Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.11.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.3.23)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.13.4)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.4.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (10.0.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.11.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.98)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=de96405a0ef91a98c0fe042dab0dcb400fd9c5cd60e60ca43920ad44bce94913\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport numpy as np\nimport datasets\n\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n)\n\nfrom tabulate import tabulate\nimport nltk\nfrom datetime import datetime\n\nimport datasets\nfrom datasets import Dataset, DatasetDict","metadata":{"id":"5TsWSgrcJfFH","execution":{"iopub.status.busy":"2023-05-22T14:05:45.825224Z","iopub.execute_input":"2023-05-22T14:05:45.825687Z","iopub.status.idle":"2023-05-22T14:05:59.547206Z","shell.execute_reply.started":"2023-05-22T14:05:45.825648Z","shell.execute_reply":"2023-05-22T14:05:59.546260Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model and tokenizer","metadata":{"id":"aX-q_O-hoe3g"}},{"cell_type":"code","source":"model_name = \"facebook/bart-base\"\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# tokenization\nencoder_max_length = 512 \ndecoder_max_length = 64","metadata":{"id":"7vMhyyIPobyx","outputId":"0a7314f2-8e7f-40e5-d381-82a741f5f318","execution":{"iopub.status.busy":"2023-05-22T14:05:59.548740Z","iopub.execute_input":"2023-05-22T14:05:59.549083Z","iopub.status.idle":"2023-05-22T14:06:08.444205Z","shell.execute_reply.started":"2023-05-22T14:05:59.549049Z","shell.execute_reply":"2023-05-22T14:06:08.443189Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f658ec4d52b489fb866b49eaca2c698"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61a4f6aa98244d6fb2a52d0f423f9f06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59c140c4a4b447a98694733351a1d9bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c19c34453b574a3187e638e6fbbbb414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb47a5f098644664bef0fecf3c39e0c5"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Prepare data","metadata":{"id":"HobbZlV4JaRr"}},{"cell_type":"code","source":"df_train = pd.read_csv(f'{PATH}/PRTiger/no-token/train.csv')\ndf_valid = pd.read_csv(f'{PATH}/PRTiger/no-token/valid.csv')\ndf_test = pd.read_csv(f'{PATH}/PRTiger/no-token/test.csv')","metadata":{"id":"bNeZwJXJJoMV","execution":{"iopub.status.busy":"2023-05-22T14:06:08.445893Z","iopub.execute_input":"2023-05-22T14:06:08.446259Z","iopub.status.idle":"2023-05-22T14:06:08.769850Z","shell.execute_reply.started":"2023-05-22T14:06:08.446225Z","shell.execute_reply":"2023-05-22T14:06:08.768650Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def format_data(df_input):\n  df_input = df_input[['text', 'summary']]\n  df_input.columns = [\"document\", \"summary\"]\n  df_input['document'] = df_input['document'].str.lower()\n  df_input['summary'] = df_input['summary'].str.lower()\n  return df_input","metadata":{"id":"EGfPjjaaNoYN","execution":{"iopub.status.busy":"2023-05-22T14:06:08.773490Z","iopub.execute_input":"2023-05-22T14:06:08.773865Z","iopub.status.idle":"2023-05-22T14:06:08.784490Z","shell.execute_reply.started":"2023-05-22T14:06:08.773830Z","shell.execute_reply":"2023-05-22T14:06:08.783501Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train = format_data(df_train)\nvalidation = format_data(df_valid)\ntest = format_data(df_test)","metadata":{"id":"dQovS10emSRc","outputId":"e6b67bf9-5d32-43db-9589-5fc6a6b307db","execution":{"iopub.status.busy":"2023-05-22T14:06:08.790705Z","iopub.execute_input":"2023-05-22T14:06:08.791306Z","iopub.status.idle":"2023-05-22T14:06:08.914584Z","shell.execute_reply.started":"2023-05-22T14:06:08.791252Z","shell.execute_reply":"2023-05-22T14:06:08.913468Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3493555275.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_input['document'] = df_input['document'].str.lower()\n/tmp/ipykernel_31/3493555275.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_input['summary'] = df_input['summary'].str.lower()\n/tmp/ipykernel_31/3493555275.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_input['document'] = df_input['document'].str.lower()\n/tmp/ipykernel_31/3493555275.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_input['summary'] = df_input['summary'].str.lower()\n/tmp/ipykernel_31/3493555275.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_input['document'] = df_input['document'].str.lower()\n/tmp/ipykernel_31/3493555275.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_input['summary'] = df_input['summary'].str.lower()\n","output_type":"stream"}]},{"cell_type":"code","source":"train_data_txt = Dataset.from_pandas(train)\nvalidation_data_txt = Dataset.from_pandas(validation)\ntest_data_txt = Dataset.from_pandas(test)","metadata":{"id":"ZNKpDF6oprUp","execution":{"iopub.status.busy":"2023-05-22T14:06:08.916150Z","iopub.execute_input":"2023-05-22T14:06:08.917225Z","iopub.status.idle":"2023-05-22T14:06:09.002248Z","shell.execute_reply.started":"2023-05-22T14:06:08.917187Z","shell.execute_reply":"2023-05-22T14:06:09.001338Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize","metadata":{"id":"5pbe750YpMfD"}},{"cell_type":"code","source":"def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n    source, target = batch[\"document\"], batch[\"summary\"]\n    source_tokenized = tokenizer(\n        source, padding=\"max_length\", truncation=True, max_length=max_source_length\n    )\n    target_tokenized = tokenizer(\n        target, padding=\"max_length\", truncation=True, max_length=max_target_length\n    )\n\n    batch = {k: v for k, v in source_tokenized.items()}\n    # Ignore padding in the loss\n    batch[\"labels\"] = [\n        [-100 if token == tokenizer.pad_token_id else token for token in l]\n        for l in target_tokenized[\"input_ids\"]\n    ]\n    return batch\n\n\ntrain_data = train_data_txt.map(\n    lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=train_data_txt.column_names,\n)\n\nvalidation_data = validation_data_txt.map(\n    lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=validation_data_txt.column_names,\n)\n\n\ntest_data = test_data_txt.map(\n    lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=validation_data_txt.column_names,\n)","metadata":{"id":"PyksYNwxA4OM","outputId":"5df05de4-a3b3-4bac-cd44-ebdce845b081","execution":{"iopub.status.busy":"2023-05-22T14:06:09.003658Z","iopub.execute_input":"2023-05-22T14:06:09.004004Z","iopub.status.idle":"2023-05-22T14:06:43.112868Z","shell.execute_reply.started":"2023-05-22T14:06:09.003973Z","shell.execute_reply":"2023-05-22T14:06:43.111938Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/36 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d39630b7f8fa45049fa028ff6f0ce3a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5034fc3e1827416894b5218ff26a0553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c78b6d3c7a545feac96b35f2a82f6c3"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Train Setting","metadata":{"id":"h7ViBmMopWfb"}},{"cell_type":"markdown","source":"## Metrics","metadata":{"id":"9EfTztMPv2vG"}},{"cell_type":"code","source":"# Borrowed from https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_summarization.py\n\nnltk.download(\"punkt\", quiet=True)\n\nmetric = datasets.load_metric(\"rouge\")\n\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # rougeLSum expects newline after each sentence\n    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n    return preds, labels\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(\n        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n    )\n    # Extract a few results from ROUGE\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    prediction_lens = [\n        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n    ]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"id":"rpNCGl2sYl2p","outputId":"d56233c8-1c55-4d38-a7ea-8eaf7a3fc4df","execution":{"iopub.status.busy":"2023-05-22T14:06:43.115120Z","iopub.execute_input":"2023-05-22T14:06:43.116178Z","iopub.status.idle":"2023-05-22T14:06:43.852997Z","shell.execute_reply.started":"2023-05-22T14:06:43.116141Z","shell.execute_reply":"2023-05-22T14:06:43.852105Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"370cdfe0c3414f7798640721ed93be5d"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Training arguments","metadata":{"id":"8O1EeUi-pbPA"}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"baseline\",\n    seed = 42,\n    data_seed = 42,\n    num_train_epochs=4, \n    do_train=True,\n    do_eval=True,\n    per_device_train_batch_size=4, \n    per_device_eval_batch_size=4,\n    warmup_steps=500,\n    weight_decay=0.1,\n    label_smoothing_factor=0.1,\n    predict_with_generate=True,\n    logging_steps=6000,\n    evaluation_strategy =\"steps\",\n    eval_steps = 6000, \n    save_steps = 6000,\n    save_total_limit = 5, \n    load_best_model_at_end=True,\n    report_to=\"none\", \n\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n    eval_dataset=validation_data,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"id":"6R9d7ELIpX9F","execution":{"iopub.status.busy":"2023-05-22T14:06:43.854586Z","iopub.execute_input":"2023-05-22T14:06:43.855000Z","iopub.status.idle":"2023-05-22T14:06:48.613320Z","shell.execute_reply.started":"2023-05-22T14:06:43.854963Z","shell.execute_reply":"2023-05-22T14:06:48.612338Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{"id":"nkRb7hvgPrf2"}},{"cell_type":"code","source":"WANDB_INTEGRATION = False","metadata":{"execution":{"iopub.status.busy":"2023-05-22T14:06:48.614759Z","iopub.execute_input":"2023-05-22T14:06:48.615117Z","iopub.status.idle":"2023-05-22T14:06:48.620231Z","shell.execute_reply.started":"2023-05-22T14:06:48.615081Z","shell.execute_reply":"2023-05-22T14:06:48.618960Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"qYcYcbkr7ZZD","outputId":"6753808d-c636-4c2a-c20a-817958f3b472","execution":{"iopub.status.busy":"2023-05-22T14:06:48.621745Z","iopub.execute_input":"2023-05-22T14:06:48.622383Z","iopub.status.idle":"2023-05-22T16:30:30.128702Z","shell.execute_reply.started":"2023-05-22T14:06:48.622351Z","shell.execute_reply":"2023-05-22T16:30:30.127694Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nYou're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='35052' max='35052' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [35052/35052 2:23:39, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n      <th>Gen Len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>6000</td>\n      <td>3.981000</td>\n      <td>3.623652</td>\n      <td>43.681100</td>\n      <td>22.857000</td>\n      <td>40.206300</td>\n      <td>40.204600</td>\n      <td>13.374900</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>3.575300</td>\n      <td>3.527457</td>\n      <td>45.272600</td>\n      <td>23.743400</td>\n      <td>41.673200</td>\n      <td>41.685900</td>\n      <td>13.108400</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>3.345000</td>\n      <td>3.493475</td>\n      <td>46.076200</td>\n      <td>24.673800</td>\n      <td>42.195000</td>\n      <td>42.214600</td>\n      <td>13.615000</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>3.059700</td>\n      <td>3.438034</td>\n      <td>45.848300</td>\n      <td>24.481800</td>\n      <td>42.265000</td>\n      <td>42.299100</td>\n      <td>13.170500</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>2.907900</td>\n      <td>3.469506</td>\n      <td>46.549000</td>\n      <td>24.713500</td>\n      <td>42.812900</td>\n      <td>42.828500</td>\n      <td>13.199200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=35052, training_loss=3.291603646457406, metrics={'train_runtime': 8621.4603, 'train_samples_per_second': 16.263, 'train_steps_per_second': 4.066, 'total_flos': 4.274496466845696e+16, 'train_loss': 3.291603646457406, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluate Valid","metadata":{"id":"V3C-4SfOPssY"}},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"id":"_-QyUtCRH9DO","execution":{"iopub.status.busy":"2023-05-22T16:30:30.130332Z","iopub.execute_input":"2023-05-22T16:30:30.130706Z","iopub.status.idle":"2023-05-22T16:36:16.909190Z","shell.execute_reply.started":"2023-05-22T16:30:30.130673Z","shell.execute_reply":"2023-05-22T16:36:16.908277Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1096' max='1096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1096/1096 05:39]\n    </div>\n    "},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 3.4380335807800293,\n 'eval_rouge1': 45.8483,\n 'eval_rouge2': 24.4818,\n 'eval_rougeL': 42.265,\n 'eval_rougeLsum': 42.2991,\n 'eval_gen_len': 13.1705,\n 'eval_runtime': 346.7671,\n 'eval_samples_per_second': 12.637,\n 'eval_steps_per_second': 3.161,\n 'epoch': 4.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Save Model","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/baseline\")","metadata":{"id":"3XmRylPWba-U","execution":{"iopub.status.busy":"2023-05-22T16:36:16.910802Z","iopub.execute_input":"2023-05-22T16:36:16.911175Z","iopub.status.idle":"2023-05-22T16:36:18.034493Z","shell.execute_reply.started":"2023-05-22T16:36:16.911142Z","shell.execute_reply":"2023-05-22T16:36:18.033523Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"!zip -r baseline1.zip /kaggle/working/baseline","metadata":{"execution":{"iopub.status.busy":"2023-05-22T16:36:18.035837Z","iopub.execute_input":"2023-05-22T16:36:18.036646Z","iopub.status.idle":"2023-05-22T16:44:20.998358Z","shell.execute_reply.started":"2023-05-22T16:36:18.036610Z","shell.execute_reply":"2023-05-22T16:44:20.997126Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n  adding: kaggle/working/baseline/ (stored 0%)\n  adding: kaggle/working/baseline/special_tokens_map.json (deflated 52%)\n  adding: kaggle/working/baseline/checkpoint-30000/ (stored 0%)\n  adding: kaggle/working/baseline/checkpoint-30000/scheduler.pt (deflated 48%)\n  adding: kaggle/working/baseline/checkpoint-30000/trainer_state.json (deflated 73%)\n  adding: kaggle/working/baseline/checkpoint-30000/special_tokens_map.json (deflated 52%)\n  adding: kaggle/working/baseline/checkpoint-30000/tokenizer_config.json (deflated 50%)\n  adding: kaggle/working/baseline/checkpoint-30000/pytorch_model.bin (deflated 8%)\n  adding: kaggle/working/baseline/checkpoint-30000/vocab.json (deflated 59%)\n  adding: kaggle/working/baseline/checkpoint-30000/optimizer.pt (deflated 9%)\n  adding: kaggle/working/baseline/checkpoint-30000/tokenizer.json (deflated 72%)\n  adding: kaggle/working/baseline/checkpoint-30000/generation_config.json (deflated 45%)\n  adding: kaggle/working/baseline/checkpoint-30000/merges.txt (deflated 53%)\n  adding: kaggle/working/baseline/checkpoint-30000/rng_state.pth (deflated 28%)\n  adding: kaggle/working/baseline/checkpoint-30000/config.json (deflated 64%)\n  adding: kaggle/working/baseline/checkpoint-30000/training_args.bin (deflated 49%)\n  adding: kaggle/working/baseline/checkpoint-12000/ (stored 0%)\n  adding: kaggle/working/baseline/checkpoint-12000/scheduler.pt (deflated 49%)\n  adding: kaggle/working/baseline/checkpoint-12000/trainer_state.json (deflated 63%)\n  adding: kaggle/working/baseline/checkpoint-12000/special_tokens_map.json (deflated 52%)\n  adding: kaggle/working/baseline/checkpoint-12000/tokenizer_config.json (deflated 50%)\n  adding: kaggle/working/baseline/checkpoint-12000/pytorch_model.bin (deflated 8%)\n  adding: kaggle/working/baseline/checkpoint-12000/vocab.json (deflated 59%)\n  adding: kaggle/working/baseline/checkpoint-12000/optimizer.pt (deflated 9%)\n  adding: kaggle/working/baseline/checkpoint-12000/tokenizer.json (deflated 72%)\n  adding: kaggle/working/baseline/checkpoint-12000/generation_config.json (deflated 45%)\n  adding: kaggle/working/baseline/checkpoint-12000/merges.txt (deflated 53%)\n  adding: kaggle/working/baseline/checkpoint-12000/rng_state.pth (deflated 27%)\n  adding: kaggle/working/baseline/checkpoint-12000/config.json (deflated 64%)\n  adding: kaggle/working/baseline/checkpoint-12000/training_args.bin (deflated 49%)\n  adding: kaggle/working/baseline/checkpoint-6000/ (stored 0%)\n  adding: kaggle/working/baseline/checkpoint-6000/scheduler.pt (deflated 49%)\n  adding: kaggle/working/baseline/checkpoint-6000/trainer_state.json (deflated 55%)\n  adding: kaggle/working/baseline/checkpoint-6000/special_tokens_map.json (deflated 52%)\n  adding: kaggle/working/baseline/checkpoint-6000/tokenizer_config.json (deflated 50%)\n  adding: kaggle/working/baseline/checkpoint-6000/pytorch_model.bin (deflated 8%)\n  adding: kaggle/working/baseline/checkpoint-6000/vocab.json (deflated 59%)\n  adding: kaggle/working/baseline/checkpoint-6000/optimizer.pt (deflated 9%)\n  adding: kaggle/working/baseline/checkpoint-6000/tokenizer.json (deflated 72%)\n  adding: kaggle/working/baseline/checkpoint-6000/generation_config.json (deflated 45%)\n  adding: kaggle/working/baseline/checkpoint-6000/merges.txt (deflated 53%)\n  adding: kaggle/working/baseline/checkpoint-6000/rng_state.pth (deflated 28%)\n  adding: kaggle/working/baseline/checkpoint-6000/config.json (deflated 64%)\n  adding: kaggle/working/baseline/checkpoint-6000/training_args.bin (deflated 49%)\n  adding: kaggle/working/baseline/tokenizer_config.json (deflated 50%)\n  adding: kaggle/working/baseline/checkpoint-18000/ (stored 0%)\n  adding: kaggle/working/baseline/checkpoint-18000/scheduler.pt (deflated 48%)\n  adding: kaggle/working/baseline/checkpoint-18000/trainer_state.json (deflated 68%)\n  adding: kaggle/working/baseline/checkpoint-18000/special_tokens_map.json (deflated 52%)\n  adding: kaggle/working/baseline/checkpoint-18000/tokenizer_config.json (deflated 50%)\n  adding: kaggle/working/baseline/checkpoint-18000/pytorch_model.bin (deflated 8%)\n  adding: kaggle/working/baseline/checkpoint-18000/vocab.json (deflated 59%)\n  adding: kaggle/working/baseline/checkpoint-18000/optimizer.pt (deflated 9%)\n  adding: kaggle/working/baseline/checkpoint-18000/tokenizer.json (deflated 72%)\n  adding: kaggle/working/baseline/checkpoint-18000/generation_config.json (deflated 45%)\n  adding: kaggle/working/baseline/checkpoint-18000/merges.txt (deflated 53%)\n  adding: kaggle/working/baseline/checkpoint-18000/rng_state.pth (deflated 27%)\n  adding: kaggle/working/baseline/checkpoint-18000/config.json (deflated 64%)\n  adding: kaggle/working/baseline/checkpoint-18000/training_args.bin (deflated 49%)\n  adding: kaggle/working/baseline/pytorch_model.bin (deflated 8%)\n  adding: kaggle/working/baseline/checkpoint-24000/ (stored 0%)\n  adding: kaggle/working/baseline/checkpoint-24000/scheduler.pt (deflated 48%)\n  adding: kaggle/working/baseline/checkpoint-24000/trainer_state.json (deflated 71%)\n  adding: kaggle/working/baseline/checkpoint-24000/special_tokens_map.json (deflated 52%)\n  adding: kaggle/working/baseline/checkpoint-24000/tokenizer_config.json (deflated 50%)\n  adding: kaggle/working/baseline/checkpoint-24000/pytorch_model.bin (deflated 8%)\n  adding: kaggle/working/baseline/checkpoint-24000/vocab.json (deflated 59%)\n  adding: kaggle/working/baseline/checkpoint-24000/optimizer.pt (deflated 9%)\n  adding: kaggle/working/baseline/checkpoint-24000/tokenizer.json (deflated 72%)\n  adding: kaggle/working/baseline/checkpoint-24000/generation_config.json (deflated 45%)\n  adding: kaggle/working/baseline/checkpoint-24000/merges.txt (deflated 53%)\n  adding: kaggle/working/baseline/checkpoint-24000/rng_state.pth (deflated 28%)\n  adding: kaggle/working/baseline/checkpoint-24000/config.json (deflated 64%)\n  adding: kaggle/working/baseline/checkpoint-24000/training_args.bin (deflated 49%)\n  adding: kaggle/working/baseline/vocab.json (deflated 59%)\n  adding: kaggle/working/baseline/tokenizer.json (deflated 72%)\n  adding: kaggle/working/baseline/generation_config.json (deflated 45%)\n  adding: kaggle/working/baseline/merges.txt (deflated 53%)\n  adding: kaggle/working/baseline/config.json (deflated 64%)\n  adding: kaggle/working/baseline/training_args.bin (deflated 49%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluate Test","metadata":{}},{"cell_type":"code","source":"tester = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","metadata":{"id":"7zw8GnLHba65","execution":{"iopub.status.busy":"2023-05-22T16:44:21.000691Z","iopub.execute_input":"2023-05-22T16:44:21.001713Z","iopub.status.idle":"2023-05-22T16:44:21.017486Z","shell.execute_reply.started":"2023-05-22T16:44:21.001671Z","shell.execute_reply":"2023-05-22T16:44:21.016115Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"tester.evaluate()","metadata":{"id":"knoTILrKpS_S","execution":{"iopub.status.busy":"2023-05-22T16:44:21.019113Z","iopub.execute_input":"2023-05-22T16:44:21.019638Z","iopub.status.idle":"2023-05-22T16:50:09.243373Z","shell.execute_reply.started":"2023-05-22T16:44:21.019599Z","shell.execute_reply":"2023-05-22T16:50:09.242328Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1096' max='1096' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1096/1096 05:41]\n    </div>\n    "},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 3.4328579902648926,\n 'eval_rouge1': 46.6314,\n 'eval_rouge2': 24.9313,\n 'eval_rougeL': 42.7895,\n 'eval_rougeLsum': 42.7886,\n 'eval_gen_len': 13.1917,\n 'eval_runtime': 348.2106,\n 'eval_samples_per_second': 12.584,\n 'eval_steps_per_second': 3.148}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluation","metadata":{"id":"-gSLVnGL9bol"}},{"cell_type":"markdown","source":"**Valid**","metadata":{"id":"-hWa2Tnvv5R8"}},{"cell_type":"code","source":"def generate_summary(test_samples, model):\n    inputs = tokenizer(\n        test_samples[\"document\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=encoder_max_length,\n        return_tensors=\"pt\",\n    )\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    outputs = model.generate(input_ids, attention_mask=attention_mask)\n    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return outputs, output_str\n\n\n# model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nvalidation_samples = validation_data_txt.select(range(16))\n\n# summaries_before_tuning = generate_summary(validation_samples, model_before_tuning)[1]\nsummaries_after_tuning = generate_summary(validation_samples, model)[1]","metadata":{"id":"NV64-XdA_rOM","execution":{"iopub.status.busy":"2023-05-22T16:50:09.244935Z","iopub.execute_input":"2023-05-22T16:50:09.245302Z","iopub.status.idle":"2023-05-22T16:50:09.947837Z","shell.execute_reply.started":"2023-05-22T16:50:09.245253Z","shell.execute_reply":"2023-05-22T16:50:09.946678Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\n    tabulate(\n        zip(\n            range(len(summaries_after_tuning)),\n            summaries_after_tuning,\n            validation_samples[\"summary\"],\n        ),\n        headers=[\"Id\", \"Summary after\", \"Summary before\"],\n    )\n)\nprint(\"\\nTarget summaries:\\n\")\nprint(\n    tabulate(list(enumerate(validation_samples[\"summary\"])), headers=[\"Id\", \"Target summary\"])\n)\nprint(\"\\nSource documents:\\n\")\nprint(tabulate(list(enumerate(validation_samples[\"document\"])), headers=[\"Id\", \"Document\"]))","metadata":{"id":"D7IPtJLjCcmS","execution":{"iopub.status.busy":"2023-05-22T16:50:09.949951Z","iopub.execute_input":"2023-05-22T16:50:09.950372Z","iopub.status.idle":"2023-05-22T16:50:09.984192Z","shell.execute_reply.started":"2023-05-22T16:50:09.950333Z","shell.execute_reply":"2023-05-22T16:50:09.983313Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"  Id  Summary after                                                              Summary before\n----  -------------------------------------------------------------------------  -----------------------------------------------------------------------------------\n   0  fix error when service type is nodeport                                    fix notes.txt when service type is nodeport\n   1  add rotationangle prop to react-swipeable                                  add rotationangle prop to react-swipeable\n   2  set max-width on search box and selection area                             fix long text causing the search box and selections to overflow on multiple selects\n   3  cherry pick #14203 to 20.7: fix issue #14202                               cherry pick #14203 to 20.6: fix issue #14202\n   4  clarify null and duplicates in javadocs                                    explicit handling of null values with retainduplicates\n   5  implement basedtypetests for arrowstringdtype                              stringdtype parameterized by storage (python or pyarrow)\n   6  implement gl_linear texture filter and allow zooming via mouse wheel       add bilinear texture sampling and more texture options for 3dfileviewer\n   7  consolidate sample rates and add hard reset gpio                           audioinjector octo sound card, updates\n   8  make optunasearch define-by-run space incompatible with evaluated_rewards  check compatibility of search space with evaluated_rewards\n   9  fix gpt2 and ctrl attention masks                                          fix input ids can be none attn mask\n  10  adding redux reliance to v2.0.0                                            update wepy-redux typing to add getstore / setstore\n  11  add @spiorder support and update spi loader                                add spi precedence support and apply for transport spi\n  12  make preg_*() to yield null instead of '' for unset substrings             distinguish between unmatched subpatterns and empty matches in preg_*()\n  13  implement reference counting for http reading                              add byte array pooling to nio http transport\n  14  create docker registry api client libraries                                improve docs with note about registry frontend web app\n  15  allow user to pack 32 bit float data in to 16bit float data                add feature to convert fp32(float) to fp16(half) using hw instructions\n\nTarget summaries:\n\n  Id  Target summary\n----  -----------------------------------------------------------------------------------\n   0  fix notes.txt when service type is nodeport\n   1  add rotationangle prop to react-swipeable\n   2  fix long text causing the search box and selections to overflow on multiple selects\n   3  cherry pick #14203 to 20.6: fix issue #14202\n   4  explicit handling of null values with retainduplicates\n   5  stringdtype parameterized by storage (python or pyarrow)\n   6  add bilinear texture sampling and more texture options for 3dfileviewer\n   7  audioinjector octo sound card, updates\n   8  check compatibility of search space with evaluated_rewards\n   9  fix input ids can be none attn mask\n  10  update wepy-redux typing to add getstore / setstore\n  11  add spi precedence support and apply for transport spi\n  12  distinguish between unmatched subpatterns and empty matches in preg_*()\n  13  add byte array pooling to nio http transport\n  14  improve docs with note about registry frontend web app\n  15  add feature to convert fp32(float) to fp16(half) using hw instructions\n\nSource documents:\n\n  Id  Document\n----  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   0  when service type is nodeport, install will failed with error: render error in \"graylog/templates/notes.txt\": template: graylog/templates/notes.txt:9:131: executing \"graylog/templates/notes.txt\" at <{{template \"fullname...>: template \"fullname\" not defined dco signed   [stable/graylog] add namespace in notes.txt when get secret   [stable/graylog] increase chart version to 0.1.2   merge remote-tracking branch 'upstream/master'   [stable/graylog] fix error when service type is nodeport\n   1  add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). the package does not already provide its own types, or cannot have its .d.ts files generated via --declaration create it with dts-gen --dt, not by basing it on an existing project. tslint.json should be present, and tsconfig.json should have noimplicitany, noimplicitthis, strictnullchecks, and strictfunctiontypes set to true. provide a url to documentation or source code which provides context for the suggested changes: formidablelabs/react-swipeable#103 increase the version number in the header if appropriate. if you are making substantial changes, consider adding a tslint.json containing { \"extends\": \"dtslint/dt.json\" }. if a package was never on definitelytyped, you don't need to do anything. (if you wrote a package and provided types, you don't need to register it with us.) add it to notneededpackages.json.   add rotationangle prop   add rotationangle prop to test\n   2  set max-width on search box and selection area pinned the \"clear\" icon to the top right on multiple selects fixes #5863.   switch clear button to no longer be floated   this also adds a consistent padding to the right side of the selection   area to allow the clear button to be absolutely positioned on the right   side without having to worry about it colliding with one of the   selected elements. this is not ideal since it always shows up, even   when the clear button is not enabled. this is also not ideal since   it maintains the padding for the second row and beyond, even though   the clear button is not visible on those rows.   we can see if we can work through those issues later. but for now   it's working consistent in both chrome and ie.   set max-width to 100% on search box   this should keep the search box set so it does not overflow the   container that it is held within. so when people search for incredibly   long strings, it properly maintains width and does not break out of   the container and cause rendering issues.   set max-width on the individual selections   this sets the max width on the individual selections such that   selections with abnormally long width will not exceed the size of   the container and break rendering. in order to remain consistent with   how the display is done for a single select, we do not allow the   contents of selection in a multiple select to break across lines.   in order to fix a rendering issue that occurs when the overflow   is set to anything other than visible on the list item, we set   vertical-align to baseline to ensure that all of the list items   are vertically aligned to the same location. if we don't set this, a   channel that is roughly 5px tall starts to form between the rows. it's   not clear what causes this channel to form but the current solution   for working around it is to correct the vertical alignment which   appears to cause it to collapse and not form.   4.1.0 beta text wrapping or not aligned.\n   3  original pull-request #14203 this pull-request is a first step of an automated backporting. it contains changes like after calling a local command git cherry-pick. if you intend to continue backporting this changes, then resolve all conflicts if any. otherwise, if you do not want to backport them, then just close this pull-request. the check results does not matter at this step - you can safely ignore them. also this pull-request will be merged automatically as it reaches the mergeable state, but you always can merge it manually.   fix issue #14202   testcase for issue 14202   fix issue #14202\n   4  in general the behavior of window stores with retainduplicates is not well documented or enforced, so we should attempt to clarify things better in the javadocs and in the code itself. this explicitly skips the put/delete when the value is null and duplicates are allowed, and specifies this behavior in the docs. also adds in a test i left out in the earlier pr #8564   add missing test, clarify nulls in jvadocs   skip delete with duplicates\n   5  continuation of #36142 todo: during the merge i removed the import gymnastics from pandas/core/arrays/string_arrow.py. we added since arrowstringdtype should always be available. i think this is no longer the case and we want to move the import checks to stringdtype. i expect envs without pyarrow to fail for now. test_arrow_roundtrip is failing. stringdtype.__from_arrow__ needs updating. address @jorisvandenbossche comments #36142 (comment) and #36142 (comment) some other test failures assertionerror: assert 'unknown-array' == 'string'   implement basedtypetests for arrowstringdtype   refactor to use parametrized stringdtype   abs-imports   post merge fixup\n   6  this adds bilinear filtering mode for the texture sampler, adds an option to the texture menu in 3dfileviewer to set magnification filters and allows zooming in/out via mouse wheel.   libgl: implement gl_linear texture filter   3dfileviewer: add magnification filters to texture menu   3dfileviewer: allow zooming via mouse wheel\n   7  these two patches add regulators for the cs42448 codec, constrain the sample rates and add a hard reset gpio for the sound card.   audioinjector addons dts : add reset and regulators for the octo product.   this patch adds a reset gpio for the audioinjector.net octo sound card.   this patch adds missing regulators for the cs42448 codec.   audioinjector octo : consolidate sample rates and add a codec reset.   this patch consolidates the sample rates which the audioinjector.net octo   sound card can use. the consolidation of sample rates are due to the   capabilities of the cs42448 codec.   this codec also requires a hard reset using the gpio pin 5 upon probe.\n   8  optunasearch accepts both distribution dictionary or define-by-run as a definition for the search space. the latter should not be used, when evaluated_rewards is provided. creation of trials in optuna only allows the distribution strategy. this pr adds docs for this case and provides a meaningful error when define-by-run is used with evaluated_rewards closes #18587 i've run scripts/format.sh to lint the changes in this pr. i need some guidance how to run the tests. is there guidance on how to execute the tests? installing the dependencies inside the python folder and running python -m unittest fails to execute the tests: importerror: failed to import test module: ray.   [tune]: optunasearch check space compatibility   run scripts/format.sh   [tune]: optunasearch define-by-run space incompatible with evaluated_rewards\n   9  make sure batch_size is correct for gpt2 and ctrl - these models need a slightly different behavior since the shape of input_ids can change depending on whether the past variable is inserted or not. see also: pr: #3033 and its issue: #3031   fix issue 3289   fix attention mask if input_ids none behavior   gpt-2 attention_mask reshaping uses input_ids first dimension\n  10  add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). provide a url to documentation or source code which provides context for the suggested changes: <> increase the version number in the header if appropriate. if you are making substantial changes, consider adding a tslint.json containing { \"extends\": \"dtslint/dt.json\" }.   adding redux reliance   adding redux reliance\n  11  add @spiorder support and apply precedence support for transport spi (commandcenter, heartbeatsender). none add a @spiorder annotation to indicate precedence. also update spiloader to support get the spi with the highest precedence. add precedence support for transport spi (commandcenter, heartbeatsender). two providers now will choose the instance with the highest precedence. run a demo, put with two different transport modules, then see whether the spi of higher order is activated. none   add @spiorder annotation and update spi loader for loading spi with highest precedence   make commandcenterprovider and heartbeatsenderprovider choose the instance with highest precedence\n  12  currently it is not easily possible to distinguish between unmatched subpatterns (i.e. unset substrings) and empty matches in preg_match(_all); in both cases an empty string is yielded. using preg_offset_capture makes that possible (negative offset indicate unmatched subpatterns), but that seems to be unnecessarily clumsy. in bug #61780 it has been proposed not to set the respective keys in $matches, but that might be too big a bc break even for php 7.0.0. setting those to null, however, might be acceptable, and would still allow to distinguish between the two cases (even with isset). therefore i think this pr would resolve bug #61780.   added failing tests for bug #61780   changed preg_*() to yield null instead of '' for unset substrings   adjusted other tests to cater to changed behavior   added test to clearly show distinction between unset (aka unmatched) subpattern and an empty match\n  13  this is related to #28898. this pr implements pooling of bytes arrays when reading from the wire in the http server transport. in order to do this, we must integrate with netty reference counting. that manner in which this pr implements this is making pages in inboundchannelbuffer reference counted. when we accessing the underlying page to pass to netty, we retain the page. when netty releases its bytebuf, it releases the underlying pages we have passed to it.   work on implementing reference counting for http reading   wip   hook up reference counting   add tests   change names\n  14  hi, i am the author of the docker-registry-frontend web app. it is a web application for browsing and manipulating a docker registry by using the docker registry api and not the docker remote api. that is why i've create an extra page in the docker documentation under \"reference -> docker registry api client libraries\". this is where i started listing libraries and webapps just like on the \"reference -> docker remote api client libraries\". the only exception is that those libraries deal with the docker registry api. please, accept my pull request or let me know what else i can do for the pr to be accepted. since i only edited a few lines in the documentation, i consider my changes an exception to the normal contribution guidelines. but let me know if you think different. thank you in advance konrad   create registry_api_client_libraries.md   add registry_api_client_libraries.md to mkdocs.yml\n  15  former  #6447 by @tomoaki0705 what does this pr change? allow user to pack 32 bit float data in to 16bit float data (half) check compiler support check hw support before executing add test doing round trip conversion from / to fp32 treat array correctly if size is not multiple of 4 add declaration to prevent warning   add feature to convert fp32(float) to fp16(half)   * check compiler support   * check hw support before executing   * add test doing round trip conversion from / to fp32   * treat array correctly if size is not multiple of 4   * add declaration to prevent warning   * make it possible to enable fp16 on 32bit arm   * let the conversion possible on non-supported hw, too.   * add test using both hw and sw implementation   follow other interface   * remove usehw option   * update test   fix warning of doc   * update the comment to real header   fix cmake   * enable fp16 feature correctly with gcc on x86/x86_64   fix corner case when number is small   fix warning   fix to support wider compiler   * check compiler more strictly   * use gcc version of fp16 conversion if it's possible (gcc 4.7 and later)   * use current sw implementation in other cases   fix run time error on mac   * integrate hw version and sw version to same function   fix warning of build   fix run time error\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Test**","metadata":{}},{"cell_type":"code","source":"test_samples = test_data_txt.select(range(16))\ntest_summaries_after_tuning = generate_summary(test_samples, model)[1]","metadata":{"id":"XamNksY_eXOd","execution":{"iopub.status.busy":"2023-05-22T16:50:09.985429Z","iopub.execute_input":"2023-05-22T16:50:09.985754Z","iopub.status.idle":"2023-05-22T16:50:10.643596Z","shell.execute_reply.started":"2023-05-22T16:50:09.985722Z","shell.execute_reply":"2023-05-22T16:50:10.642569Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(\n    tabulate(\n        zip(\n            range(len(test_summaries_after_tuning)),\n            test_summaries_after_tuning,\n            test_samples[\"summary\"],\n        ),\n        headers=[\"Id\", \"Summary predict\", \"Summary target\"],\n    )\n)\n# print(\"\\nTarget summaries:\\n\")\n# print(\n#     tabulate(list(enumerate(test_samples[\"summary\"])), headers=[\"Id\", \"Target summary\"])\n# )\nprint(\"\\nSource documents:\\n\")\nprint(tabulate(list(enumerate(test_samples[\"document\"])), headers=[\"Id\", \"Document\"]))","metadata":{"id":"jjR8U_cbelJb","execution":{"iopub.status.busy":"2023-05-22T16:50:10.648852Z","iopub.execute_input":"2023-05-22T16:50:10.649777Z","iopub.status.idle":"2023-05-22T16:50:10.673918Z","shell.execute_reply.started":"2023-05-22T16:50:10.649740Z","shell.execute_reply":"2023-05-22T16:50:10.672964Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"  Id  Summary predict                                                                        Summary target\n----  -------------------------------------------------------------------------------------  -------------------------------------------------------------------------\n   0  fix relative paths in tests, attempt 1                                                 fix relative paths in tests, part 1\n   1  disallow null as valid parameter in get_class()                                        get_class() disallow null parameter rfc\n   2  oneclasssvm n_support returns incorrect value                                          fixed n_support_ attr for oneclasssvm and svr\n   3  use gettypecheckedbody to allow lazy type-checking when emitting function definitions  add and use abstractfunctiondecl::gettypecheckedbody\n   4  implicit generic \"any\". and specify generic parameters                                 implicit generic \"any\" for builtins\n   5  resolves checkstyle errors for lazy-loading and leader-election                        resolves checkstyle errors for api-gateway, lazy-loading, leader-election\n   6  extract static file handling to utils and make sure we handle windows well.            fix --static-dir with absolute path on windows\n   7  new perl questions and answers (#181)                                                  feature/new perl questions and answers\n   8  basic support for'return' in dy2stat                                                   add basic support for grammar 'return'\n   9  handle resize event even in full screen state                                          don't react on dpi change event on win10 >= fcu\n  10  updated typings to match vimeo's player.js v2.6.3                                      removing typings for unexposed internals\n  11  move endian flags to kbuild_helper                                                     minor endian and debug enhancement\n  12  fix document for introduction toc priority                                             fixed a problem with the translation of introduction\n  13  add and fix line-styles                                                                fixed typos and added new style options\n  14  add msgtraceenable config in transaction producer benchmark                            [issue #3162 ]add msgtraceenable config in transaction producer benchmark\n  15  d3-ease added missing easing function parameterizations                                add parameterization, strictnullchecks and  jsdoc comments\n\nSource documents:\n\n  Id  Document\n----  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n   0  relative paths in tests can point directly to .d.ts files. however, those references don't reflect what users can actually write. this pr rewrites references like './index' and \"..\" to the name of the package. ansi-styles had an unused reference, so i just deleted that one.   fix relative paths in tests, attempt 1   revert athenajs change\n   1  changes get_class() to disallow null being a valid parameter, as that behaviour is highly astonishing. the ratio of the number of characters in the rfc to the actual code change, is too darn high.   require parameter to be an object if passed.   add test.   fixed ext/standard test that calls get_class()\n   2  closes #14799 closes #15034 fixes #14774 @amueller i think this is enough? (that doesn't fix the nonsense that the attribute has size 2 (i.e. 2 classes) for oneclass and svr but that's a different issue)  it does now   fixed n_support   removed comment   add comment   oneclasssvm n_support_ returns incorrect value\n   3  refactor typecheckfunctionbodyrequest to return the type-checked body, and remove typecheckabstractfunctionbody in favor of a method on abstractfunctiondecl. in addition, start returning an errorexpr body instead of a partially type-checked body if type-checking fails. then, start using gettypecheckedbody in silgen to allow lazy type-checking when emitting function definitions.   [ast] remove newbodykind default   a bunch of callers were accidentally misusing it.   add abstractfunctiondecl::gettypecheckedbody   refactor typecheckfunctionbodyrequest to return   the type-checked body, and remove   typecheckabstractfunctionbody in favor of   a method on abstractfunctiondecl. in addition,   start returning an errorexpr body instead of   a partially type-checked body if type-checking   fails.   add some missing calls to setthrows   we no longer run error checking for already   type-checked synthesized functions, so add a   couple of setthrows calls where they were   previously missing.   [silgen] use gettypecheckedbody   use gettypecheckedbody to allow lazy   type-checking when emitting function definitions.\n   4  xref #30539   pandas\\core\\common.py:273: error: implicit generic \"any\". use \"typing.list\" and specify generic parameters   pandas\\core\\arrays\\categorical.py:514: error: implicit generic \"any\". use \"typing.list\" and specify generic parameters   pandas\\core\\indexing.py:2227: error: implicit generic \"any\". use \"typing.tuple\" and specify generic parameters   pandas\\core\\groupby\\grouper.py:422: error: implicit generic \"any\". use \"typing.dict\" and specify generic parameters   pandas\\tests\\frame\\methods\\test_replace.py:15: error: implicit generic \"any\". use \"typing.list\" and specify generic parameters   pandas\\tests\\frame\\methods\\test_replace.py:20: error: implicit generic \"any\". use \"typing.list\" and specify generic parameters   pandas\\io\\pytables.py:1462: error: implicit generic \"any\". use \"typing.list\" and specify generic parameters\n   5  reduces checkstyle errors for patterns: api-gateway lazy-loading leader-election changes involved java docs reordering imports indentations line length issues   reduces checkstyle errors in lazy-loading   reduces checkstyle errors in leader-election   reduces checkstyle errors in api-gateway\n   6  issue: #13336 primarily this fixes the handling of the -s (--static-dir) flag for windows users. this flag accepts a string in the shape of <dirname>:<endpoint>. currently it fails to handle absolute paths on windows: for a value like c:\\path it would assume c to be the dirname and \\path the endpoint. that bug is addressed here. besides fixing the bug at hand, i've extracted this piece of logic into a util, because it previously existed in two separate places. the util is now under unit test and the surrounding code has been cleaned up with some nicer logging. i've also replaced shelljs with copy from fs-extra. one less dependency is this testable with jest or chromatic screenshots? no does this need a new example in the kitchen sink apps? no does this need an update to the documentation? no   extract static file handling to utils and make sure we handle windows well.   we no longer need shelljs.\n   7  added some questions and answers in the filehandle and oop perl topics.   new questions and spell check (#181)   added new questions related with kvm, libvirt and dnf   new perl questions and answers   added some questions and answers in the filehandle and oop topics.\n   8  new features others this pr added basic support for 'return' grammar in dy2stat. it supports the control flow of 'return'. the basics idea is using a return value variable to store the early return statements and boolean state variables with if-else to skip the statements after the return statements. this pr is very basic support. there are some corner cases i didn't develop/test. for example, 'return none', 'return different length of variables', 'return non-tensor and tensor together', 'no return statement'. these corner cases will be done in my next prs. target date is this week. note: for the unit test, i changed test_program_translator.py because the staticcode of dyfunc_with_if_else will change. to guarantee the correctness of dyfunc_with_if_else, i also run it in testrecursivereturn in test_return.py. i commented the early return code in bert_dygraph_model.py because 'return different length of variables' is unsupported now. i also know that there are some other models used early return and we didn't enable it in the unit test. i will add support for it in next prs and then re-enable those tests.   add return_transformer, not tested. test=develop   test=develop   add unittest and fix some bugs before testing. test=develop\n   9  and some cosmetics related to the fix this also fixes issues with wrong rendering size after dpi changes and/or display switch off/on   [xbmc] application: handle resize event even in full screen state on windows.   [windowing] win32: update handling wm_dpichanged event\n  10  add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). provide a url to documentation or source code which provides context for the suggested changes:  increase the version number in the header if appropriate. (not applicable) if you are making substantial changes, consider adding a tslint.json containing { \"extends\": \"dtslint/dt.json\" }. (already available)   updated typings to match vimeo's player.js v2.6.3, updated tests to reflect new methods, properties, and events   cleanup of unused boilerplate lines   setplaybackrate may return rangeerror   options argument in constructor is optional   removed typings for unexposed internals   both playermap and readymap are internal weakmaps in vimeo's player.js and are never exposed, so there is no need to export their types.   merge remote-tracking branch 'upstream/master'\n  11  add debug_* constants in place of the hardcoded values. should help making self-documenting code move endian fixes to clang, previous pr was merged... fast :) i was trying to automatically insert bpf_probe_read in situations like   move endian flags to kbuild_helper   add debug constants\n  12  i hereby agree to the terms of the cla available at:  fix document for index.md and distinctive-features.md fix document for introduction toc priority detailed description / documentation draft: by adding documentation, you'll allow users to try your new feature immediately, not when someone else will have time to document it later. documentation is necessary for all features that affect user experience in any way. you can add brief documentation draft above, or add documentation right into your patch as markdown files in docs folder. if you are doing this for the first time, it's recommended to read the lightweight contributing to clickhouse documentation guide first.   fix document for index.md and distinctive-features.md   fix document for introduction toc priority\n  13  add or edit tests to reflect the change. (run with npm test.) follow the advice from the readme. avoid common mistakes. run npm run lint package-name (or tsc if no tslint.json is present). provide a url to documentation or source code which provides context for the suggested changes:  increase the version number in the header if appropriate. if you are making substantial changes, consider adding a tslint.json containing { \"extends\": \"dtslint/dt.json\" }.   add and fix line-styles   fix typo   add text-wrap style   add tests\n  14  make sure set the target branch to develop #3162 add msgtraceenable config in transaction producer benchmark follow this checklist to help us incorporate your contribution quickly and easily. notice, it would be helpful if you could finish the following 5 checklist(the last one is not necessary)before request the community to review your pr. make sure there is a github issue filed for the change (usually before you start working on it). trivial changes like typos do not require a github issue. your pull request should address just this issue, without pulling in other changes - one pr resolves one issue. format the pull request title like [issue #123] fix unknownexception when host config not exist. each commit in the pull request should have a meaningful subject line and body. write necessary unit-test(over 80% coverage) to verify your logic correction, more mock a little better when cross module dependency exist. if the new feature or significant change is committed, please remember to add integration-test in test module. run mvn -b clean apache-rat:check findbugs:findbugs checkstyle:checkstyle to make sure basic checks pass. run mvn clean install -dskipits to make sure unit-test pass. run mvn clean test-compile failsafe:integration-test  to make sure integration-test pass. if this contribution is large, please file an apache individual contributor license agreement.   fix checkstyle fail   add msgtraceenable config in transaction producer benchmark   add msgtraceenable config in transaction producer benchmark\n  15  this pr fixes #13767 adding in additional interfaces to correctly define certain easing factories, which allow parameterization (i.e. poly, back and elastic) it is related to #11365 and #11366. thanks for spotting the bug @billdwhite make your pr against the master branch. follow the advice from the readme. avoid common mistakes. run tsc without errors. run npm run lint package-name if a tslint.json is present. provide a url to documentation or source code which provides context for the suggested changes: < increase the version number in the header if appropriate. if you are making substantial changes, consider adding a tslint.json containing { \"extends\": \"../tslint.json\" }.   fetch-merge commit from dt master   merge remote-tracking branch 'definitelytyped/master'   merge remote-tracking branch 'definitelytyped/master'   merge remote-tracking branch 'definitelytyped/master'   merge remote-tracking branch 'definitelytyped/master'   merge remote-tracking branch 'definitelytyped/master'   merge remote-tracking branch 'definitelytyped/master'   d3-ease:   * [fix]: added interfaces and changed definitions for three classes of easing functions, which can be parameterized. fixes #13767.   * [enhancement]: activated strictnullchecks after validation   * [chore]: added jsdoc comments   * [chore]: added linting config file.   [bug] d3-ease omitted easing function parameterizations\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}