{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/khunanonr/bart-with-text-rank?scriptVersionId=131230070\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Load dataset","metadata":{"id":"4qQjXtKDLZ_o"}},{"cell_type":"markdown","source":"* Paper source: Automatic Pull Request Title Generation\n* Dataset: https://github.com/soarsmu/PRTiger/raw/main/data/PRTiger.zip\n* model source: https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb","metadata":{}},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-05-23T17:53:34.573603Z","iopub.execute_input":"2023-05-23T17:53:34.574122Z","iopub.status.idle":"2023-05-23T17:53:49.369218Z","shell.execute_reply.started":"2023-05-23T17:53:34.574071Z","shell.execute_reply":"2023-05-23T17:53:49.367921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown 1afDEBUClq5Oq7cSvQXLIftypYSsG8X5z\n!gdown 1Ue4U5cwz8Kt26go_X0a1x-1d_r1cPUim\n!gdown 10D00QF27gGgOaVTwzhdrQ11iqq72oV5U","metadata":{"id":"WWoB-D4pLcGK","outputId":"c8cae455-0078-44b4-b482-f1233b5b13a9","execution":{"iopub.status.busy":"2023-05-23T17:53:49.371863Z","iopub.execute_input":"2023-05-23T17:53:49.372289Z","iopub.status.idle":"2023-05-23T17:54:16.084332Z","shell.execute_reply.started":"2023-05-23T17:53:49.372246Z","shell.execute_reply":"2023-05-23T17:54:16.082884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare data","metadata":{"id":"HobbZlV4JaRr"}},{"cell_type":"code","source":"import pandas as pd","metadata":{"id":"5TsWSgrcJfFH","execution":{"iopub.status.busy":"2023-05-23T17:54:16.087834Z","iopub.execute_input":"2023-05-23T17:54:16.088273Z","iopub.status.idle":"2023-05-23T17:54:16.094215Z","shell.execute_reply.started":"2023-05-23T17:54:16.088233Z","shell.execute_reply":"2023-05-23T17:54:16.092807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/working/preprocessed_train.csv')\ndf_valid = pd.read_csv('/kaggle/working/preprocessed_validation.csv')\ndf_test = pd.read_csv('/kaggle/working/preprocessed_test.csv')\ndf_train.head()","metadata":{"id":"bNeZwJXJJoMV","outputId":"ba99617b-2076-469b-d76e-b959591affd8","execution":{"iopub.status.busy":"2023-05-23T17:54:16.09814Z","iopub.execute_input":"2023-05-23T17:54:16.098478Z","iopub.status.idle":"2023-05-23T17:54:16.468644Z","shell.execute_reply.started":"2023-05-23T17:54:16.098446Z","shell.execute_reply":"2023-05-23T17:54:16.467574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import\n---","metadata":{"id":"yUn2OqI9oPQb"}},{"cell_type":"code","source":"! pip --q install transformers\n! pip --q install datasets\n! pip --q install sentencepiece\n! pip --q install rouge_score\n# ! pip install wandb","metadata":{"id":"_gaaojSBoQ5f","outputId":"f8cf38a0-6de9-4893-fb73-383ff641a4f5","execution":{"iopub.status.busy":"2023-05-23T17:54:16.485273Z","iopub.execute_input":"2023-05-23T17:54:16.485682Z","iopub.status.idle":"2023-05-23T17:55:05.245321Z","shell.execute_reply.started":"2023-05-23T17:54:16.48564Z","shell.execute_reply":"2023-05-23T17:55:05.24387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport datasets\n\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n)\n\nfrom tabulate import tabulate\nimport nltk\nfrom datetime import datetime\n\nimport datasets\nfrom datasets import Dataset, DatasetDict","metadata":{"id":"rimUDCQGoTAJ","execution":{"iopub.status.busy":"2023-05-23T17:55:05.247649Z","iopub.execute_input":"2023-05-23T17:55:05.248082Z","iopub.status.idle":"2023-05-23T17:55:17.741006Z","shell.execute_reply.started":"2023-05-23T17:55:05.248027Z","shell.execute_reply":"2023-05-23T17:55:17.739757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# First model","metadata":{}},{"cell_type":"markdown","source":"## Model and tokenizer\n\n---","metadata":{"id":"aX-q_O-hoe3g"}},{"cell_type":"code","source":"model_name = \"facebook/bart-base\"\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel.to(\"cuda\")\n\n# tokenization\nencoder_max_length = 512  # demo\ndecoder_max_length = 64","metadata":{"id":"7vMhyyIPobyx","outputId":"56ad2488-790e-4024-b4c5-5d788a8ddcfc","execution":{"iopub.status.busy":"2023-05-23T17:55:33.56627Z","iopub.execute_input":"2023-05-23T17:55:33.56663Z","iopub.status.idle":"2023-05-23T17:55:51.423983Z","shell.execute_reply.started":"2023-05-23T17:55:33.566594Z","shell.execute_reply":"2023-05-23T17:55:51.42286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extra_token = \"<extra>\"\nend_extra_token = \"</extra>\"\nlist_tokens = [ end_extra_token, extra_token]\n\ntokenizer.add_tokens(list_tokens, special_tokens=True) ##This line is updated\ntokenizer.additional_special_tokens= list_tokens\ntokenizer.additional_special_tokens\nmodel.resize_token_embeddings(len(tokenizer))\n# model.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-05-23T17:55:51.42552Z","iopub.execute_input":"2023-05-23T17:55:51.426636Z","iopub.status.idle":"2023-05-23T17:55:51.797975Z","shell.execute_reply.started":"2023-05-23T17:55:51.426587Z","shell.execute_reply":"2023-05-23T17:55:51.796836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data\n\n---","metadata":{"id":"wwtSPRJgomBS"}},{"cell_type":"code","source":"# Pack data\ndef flatten(example):\n    return {\n        \"document\": example[\"document\"],\n        \"summary\": example[\"summary\"],\n    }\n\ndf_custom_train = Dataset.from_pandas(df_train)\ndf_custom_valid = Dataset.from_pandas(df_valid)\ndf_custom_test = Dataset.from_pandas(df_test)\n\ndf_custom_train = df_custom_train.map(flatten)\ndf_custom_valid = df_custom_valid.map(flatten)\ndf_custom_test = df_custom_test.map(flatten)","metadata":{"id":"HoN5hg-rYISn","outputId":"2dc00227-425a-4948-f065-340a463bf5d5","execution":{"iopub.status.busy":"2023-05-23T17:55:51.799508Z","iopub.execute_input":"2023-05-23T17:55:51.801507Z","iopub.status.idle":"2023-05-23T17:55:51.947189Z","shell.execute_reply.started":"2023-05-23T17:55:51.801462Z","shell.execute_reply":"2023-05-23T17:55:51.946129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess and tokenize","metadata":{"id":"5pbe750YpMfD"}},{"cell_type":"code","source":"def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n    source, target = batch[\"document\"], batch[\"summary\"]\n    source_tokenized = tokenizer(source, padding=\"max_length\", truncation=True, max_length=max_source_length )\n    target_tokenized = tokenizer(target, padding=\"max_length\", truncation=True, max_length=max_target_length )\n\n    batch = {k: v for k, v in source_tokenized.items()}\n    # Ignore padding in the loss\n    batch[\"labels\"] = [\n        [-100 if token == tokenizer.pad_token_id else token for token in l]\n        for l in target_tokenized[\"input_ids\"]\n    ]\n    return batch\n\n\ntrain_data = df_custom_train.map( lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=df_custom_train.column_names,\n)\n\nvalidation_data = df_custom_valid.map(lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=df_custom_valid.column_names,\n)\n\n\ntest_data = df_custom_test.map(lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=df_custom_test.column_names,\n)","metadata":{"id":"PyksYNwxA4OM","outputId":"c24cbd2b-50a2-491e-f0db-d6e2a7b0748c","execution":{"iopub.status.busy":"2023-05-23T17:55:51.948814Z","iopub.execute_input":"2023-05-23T17:55:51.94965Z","iopub.status.idle":"2023-05-23T17:55:52.289718Z","shell.execute_reply.started":"2023-05-23T17:55:51.949608Z","shell.execute_reply":"2023-05-23T17:55:52.28864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training\n\n---","metadata":{"id":"h7ViBmMopWfb"}},{"cell_type":"markdown","source":"### Metrics","metadata":{"id":"9EfTztMPv2vG"}},{"cell_type":"code","source":"# Borrowed from https://github.com/huggingface/transformers/blob/master/examples/seq2seq/run_summarization.py\n\nnltk.download(\"punkt\", quiet=True)\n\nmetric = datasets.load_metric(\"rouge\")\n\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # rougeLSum expects newline after each sentence\n    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n    return preds, labels\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(\n        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n    )\n    print(result.keys())\n    # Extract a few results from ROUGE\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    prediction_lens = [\n        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds\n    ]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","metadata":{"id":"rpNCGl2sYl2p","outputId":"800090b5-4f53-4a5a-bc8d-b4442921f50b","execution":{"iopub.status.busy":"2023-05-23T17:55:52.291525Z","iopub.execute_input":"2023-05-23T17:55:52.292707Z","iopub.status.idle":"2023-05-23T17:55:53.122181Z","shell.execute_reply.started":"2023-05-23T17:55:52.292664Z","shell.execute_reply":"2023-05-23T17:55:53.121209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training arguments","metadata":{"id":"8O1EeUi-pbPA"}},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"/NLP_project/bart_1\",\n    seed = 42,\n    data_seed = 42,\n    num_train_epochs=4,  # demo\n    do_train=True,\n    do_eval=True,\n    per_device_train_batch_size=4,  # demo\n    per_device_eval_batch_size=4,\n    warmup_steps=500,\n    weight_decay=0.1,\n    label_smoothing_factor=0.1,\n    predict_with_generate=True,\n    logging_dir=\"logs\",\n    logging_steps=6000,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 5,\n    save_strategy = \"steps\",\n    save_steps = 6000,\n    load_best_model_at_end=True,\n)\n\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n    eval_dataset=validation_data,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\nmodel.device","metadata":{"id":"6R9d7ELIpX9F","execution":{"iopub.status.busy":"2023-05-23T17:55:53.12363Z","iopub.execute_input":"2023-05-23T17:55:53.12413Z","iopub.status.idle":"2023-05-23T17:55:53.152448Z","shell.execute_reply.started":"2023-05-23T17:55:53.124087Z","shell.execute_reply":"2023-05-23T17:55:53.151485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{"id":"Qzcsz3gKplPO"}},{"cell_type":"code","source":"WANDB_INTEGRATION = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(trainer.evaluate())","metadata":{"id":"5yveDiz7pm3i","execution":{"iopub.status.busy":"2023-05-23T17:55:53.15469Z","iopub.execute_input":"2023-05-23T17:55:53.155489Z","iopub.status.idle":"2023-05-23T17:56:36.849272Z","shell.execute_reply.started":"2023-05-23T17:55:53.155449Z","shell.execute_reply":"2023-05-23T17:56:36.847892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"qYcYcbkr7ZZD","outputId":"fe0925cf-7f5e-494f-ece7-51a3ac332edf","execution":{"iopub.status.busy":"2023-05-23T17:56:36.854053Z","iopub.execute_input":"2023-05-23T17:56:36.856847Z","iopub.status.idle":"2023-05-23T17:56:42.572023Z","shell.execute_reply.started":"2023-05-23T17:56:36.856802Z","shell.execute_reply":"2023-05-23T17:56:42.571045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate validation","metadata":{}},{"cell_type":"code","source":"print(trainer.evaluate())","metadata":{"id":"_-QyUtCRH9DO","outputId":"dbf88edf-c643-4fcd-f560-a6cfd6e1322b","execution":{"iopub.status.busy":"2023-05-23T17:56:42.576562Z","iopub.execute_input":"2023-05-23T17:56:42.579002Z","iopub.status.idle":"2023-05-23T17:56:53.991307Z","shell.execute_reply.started":"2023-05-23T17:56:42.578959Z","shell.execute_reply":"2023-05-23T17:56:53.990217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save model","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"/kaggle/working/NLP_project/best_1_model\")","metadata":{"id":"3XmRylPWba-U","execution":{"iopub.status.busy":"2023-05-23T17:56:53.993005Z","iopub.execute_input":"2023-05-23T17:56:53.994123Z","iopub.status.idle":"2023-05-23T17:56:55.505004Z","shell.execute_reply.started":"2023-05-23T17:56:53.994076Z","shell.execute_reply":"2023-05-23T17:56:55.503869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Test","metadata":{}},{"cell_type":"code","source":"tester = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\ntester.evaluate()","metadata":{"id":"7zw8GnLHba65","execution":{"iopub.status.busy":"2023-05-23T17:56:55.50664Z","iopub.execute_input":"2023-05-23T17:56:55.507403Z","iopub.status.idle":"2023-05-23T17:57:05.773919Z","shell.execute_reply.started":"2023-05-23T17:56:55.507361Z","shell.execute_reply":"2023-05-23T17:57:05.772758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gen output","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef gen_output(model, df_custom, df_new, name, batch_size=8):\n    dataset_custom_loader = DataLoader(df_custom, batch_size=batch_size, shuffle=False)\n    first_output = []\n    for batch in dataset_custom_loader:\n        inputs = tokenizer(\n            batch[\"document\"],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=encoder_max_length,\n            return_tensors=\"pt\",\n        )\n        input_ids = inputs.input_ids.to(model.device)\n        attention_mask = inputs.attention_mask.to(model.device)\n        outputs = model.generate(input_ids, attention_mask=attention_mask)\n        output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        first_output += output_str\n    df_new[name] = first_output\n    return df_new\n\n# add new cols\ndf_train = gen_output(model, df_custom_train, df_train, \"first_output\")\ndf_valid = gen_output(model, df_custom_valid, df_valid, \"first_output\")\ndf_test = gen_output(model, df_custom_test, df_test, \"first_output\")\ndf_train","metadata":{"execution":{"iopub.status.busy":"2023-05-23T17:57:05.775492Z","iopub.execute_input":"2023-05-23T17:57:05.780738Z","iopub.status.idle":"2023-05-23T17:57:20.747222Z","shell.execute_reply.started":"2023-05-23T17:57:05.780692Z","shell.execute_reply":"2023-05-23T17:57:20.745791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample output validation\n","metadata":{"id":"-gSLVnGL9bol"}},{"cell_type":"code","source":"def generate_summary(test_samples, model):\n    inputs = tokenizer(\n        test_samples[\"document\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=encoder_max_length,\n        return_tensors=\"pt\",\n    )\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    outputs = model.generate(input_ids, attention_mask=attention_mask)\n    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return outputs, output_str\n\n\n# model_before_tuning = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nvalidation_samples = df_custom_valid.select(range(5))\n\n# summaries_before_tuning = generate_summary(validation_samples, model_before_tuning)[1]\nsummaries_after_tuning = generate_summary(validation_samples, model)[1]","metadata":{"id":"NV64-XdA_rOM","execution":{"iopub.status.busy":"2023-05-23T17:57:20.749079Z","iopub.execute_input":"2023-05-23T17:57:20.749739Z","iopub.status.idle":"2023-05-23T17:57:21.068134Z","shell.execute_reply.started":"2023-05-23T17:57:20.749697Z","shell.execute_reply":"2023-05-23T17:57:21.066838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\n    tabulate(\n        zip(\n            range(len(summaries_after_tuning)),\n            summaries_after_tuning,\n            validation_samples[\"summary\"],\n        ),\n        headers=[\"Id\", \"Summary after\", \"Summary before\"],\n    )\n)\nprint(\"\\nTarget summaries:\\n\")\nprint(\n    tabulate(list(enumerate(validation_samples[\"summary\"])), headers=[\"Id\", \"Target summary\"])\n)\nprint(\"\\nSource documents:\\n\")\nprint(tabulate(list(enumerate(validation_samples[\"document\"])), headers=[\"Id\", \"Document\"]))","metadata":{"id":"D7IPtJLjCcmS","outputId":"c7dec566-e057-4840-c1a3-cb28847739e0","execution":{"iopub.status.busy":"2023-05-23T17:57:21.070035Z","iopub.execute_input":"2023-05-23T17:57:21.070746Z","iopub.status.idle":"2023-05-23T17:57:21.097668Z","shell.execute_reply.started":"2023-05-23T17:57:21.070702Z","shell.execute_reply":"2023-05-23T17:57:21.096939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample output Test","metadata":{}},{"cell_type":"code","source":"def generate_summary(test_samples, model):\n    inputs = tokenizer(\n        test_samples[\"document\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=encoder_max_length,\n        return_tensors=\"pt\",\n    )\n    input_ids = inputs.input_ids.to(model.device)\n    attention_mask = inputs.attention_mask.to(model.device)\n    outputs = model.generate(input_ids, attention_mask=attention_mask)\n    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    return outputs, output_str\n\ntest_samples =  df_custom_test.select(range(5))\n\ntest_summaries_after_tuning = generate_summary(test_samples, model)[1]","metadata":{"id":"XamNksY_eXOd","outputId":"952d1173-fb97-4284-805b-07f5dccbb6cd","execution":{"iopub.status.busy":"2023-05-23T17:57:21.099113Z","iopub.execute_input":"2023-05-23T17:57:21.099955Z","iopub.status.idle":"2023-05-23T17:57:21.443364Z","shell.execute_reply.started":"2023-05-23T17:57:21.099913Z","shell.execute_reply":"2023-05-23T17:57:21.442067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\n    tabulate(\n        zip(\n            range(len(test_summaries_after_tuning)),\n            test_summaries_after_tuning,\n            test_samples[\"summary\"],\n        ),\n        headers=[\"Id\", \"Summary predict\", \"Summary target\"],\n    )\n)\n\nprint(\"\\nSource documents:\\n\")\nprint(tabulate(list(enumerate(test_samples[\"document\"])), headers=[\"Id\", \"Document\"]))","metadata":{"id":"jjR8U_cbelJb","outputId":"1a89d975-91db-4394-e883-0b60c6099c58","execution":{"iopub.status.busy":"2023-05-23T17:57:21.445262Z","iopub.execute_input":"2023-05-23T17:57:21.446097Z","iopub.status.idle":"2023-05-23T17:57:21.470043Z","shell.execute_reply.started":"2023-05-23T17:57:21.446051Z","shell.execute_reply":"2023-05-23T17:57:21.4685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TextRank","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nnltk.download('stopwords')","metadata":{"id":"lZXmwKreV0wn","execution":{"iopub.status.busy":"2023-05-23T17:57:21.471845Z","iopub.execute_input":"2023-05-23T17:57:21.474729Z","iopub.status.idle":"2023-05-23T17:57:21.497203Z","shell.execute_reply.started":"2023-05-23T17:57:21.474674Z","shell.execute_reply":"2023-05-23T17:57:21.49607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove*.zip","metadata":{"id":"4FyPcz7QV0IO","execution":{"iopub.status.busy":"2023-05-23T17:57:21.498707Z","iopub.execute_input":"2023-05-23T17:57:21.499483Z","iopub.status.idle":"2023-05-23T18:00:24.786787Z","shell.execute_reply.started":"2023-05-23T17:57:21.499444Z","shell.execute_reply":"2023-05-23T18:00:24.78532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract word vectors\nword_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()\nlen(word_embeddings)","metadata":{"id":"J-Q9dosgVz8A","execution":{"iopub.status.busy":"2023-05-23T18:00:24.801939Z","iopub.execute_input":"2023-05-23T18:00:24.802283Z","iopub.status.idle":"2023-05-23T18:00:36.936417Z","shell.execute_reply.started":"2023-05-23T18:00:24.802244Z","shell.execute_reply":"2023-05-23T18:00:36.932201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\nimport numpy as np\nfrom nltk import sent_tokenize, word_tokenize\n\nfrom nltk.cluster.util import cosine_distance\n\nMULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n\n\ndef normalize_whitespace(text):\n    \"\"\"\n    Translates multiple whitespace into single space character.\n    If there is at least one new line character chunk is replaced\n    by single LF (Unix new line) character.\n    \"\"\"\n    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n\n\ndef _replace_whitespace(match):\n    text = match.group()\n\n    if \"\\n\" in text or \"\\r\" in text:\n        return \"\\n\"\n    else:\n        return \" \"\n\n\ndef is_blank(string):\n    \"\"\"\n    Returns `True` if string contains only white-space characters\n    or is empty. Otherwise `False` is returned.\n    \"\"\"\n    return not string or string.isspace()\n\n\ndef get_symmetric_matrix(matrix):\n    \"\"\"\n    Get Symmetric matrix\n    :param matrix:\n    :return: matrix\n    \"\"\"\n    return matrix + matrix.T - np.diag(matrix.diagonal())\n\n\ndef core_cosine_similarity(vector1, vector2):\n    \"\"\"\n    measure cosine similarity between two vectors\n    :param vector1:\n    :param vector2:\n    :return: 0 < cosine similarity value < 1\n    \"\"\"\n    return 1 - cosine_distance(vector1, vector2)\n\n\n'''\nNote: This is not a summarization algorithm. This Algorithm pics top sentences irrespective of the order they appeared.\n'''\n\n\nclass TextRank4Sentences():\n    def __init__(self):\n        self.damping = 0.85  # damping coefficient, usually is .85\n        self.min_diff = 1e-5  # convergence threshold\n        self.steps = 100  # iteration steps\n        self.text_str = None\n        self.sentences = None\n        self.pr_vector = None\n\n    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n        if stopwords is None:\n            stopwords = []\n\n        sent1 = [w.lower() for w in sent1]\n        sent2 = [w.lower() for w in sent2]\n\n        all_words = list(set(sent1 + sent2))\n\n        vector1 = [0] * len(all_words)\n        vector2 = [0] * len(all_words)\n\n        # build the vector for the first sentence\n        for w in sent1:\n            if w in stopwords:\n                continue\n            vector1[all_words.index(w)] += 1\n\n        # build the vector for the second sentence\n        for w in sent2:\n            if w in stopwords:\n                continue\n            vector2[all_words.index(w)] += 1\n\n        return core_cosine_similarity(vector1, vector2)\n\n    def _build_similarity_matrix(self, sentences, stopwords=None):\n        # create an empty similarity matrix\n        sm = np.zeros([len(sentences), len(sentences)])\n\n        for idx1 in range(len(sentences)):\n            for idx2 in range(len(sentences)):\n                if idx1 == idx2:\n                    continue\n\n                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n\n        # Get Symmeric matrix\n        sm = get_symmetric_matrix(sm)\n\n        # Normalize matrix by column\n        norm = np.sum(sm, axis=0)\n        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is ignore the 0 element in norm\n\n        return sm_norm\n\n    def _run_page_rank(self, similarity_matrix):\n\n        pr_vector = np.array([1] * len(similarity_matrix))\n\n        # Iteration\n        previous_pr = 0\n        for epoch in range(self.steps):\n            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n            if abs(previous_pr - sum(pr_vector)) < self.min_diff:\n                break\n            else:\n                previous_pr = sum(pr_vector)\n\n        return pr_vector\n\n    def _get_sentence(self, index):\n\n        try:\n            return self.sentences[index]\n        except IndexError:\n            return \"\"\n\n    def get_top_sentences(self, number=5, concate=True):\n\n        top_sentences = []\n\n        if self.pr_vector is not None:\n\n            sorted_pr = np.argsort(self.pr_vector)\n            sorted_pr = list(sorted_pr)\n            sorted_pr.reverse()\n\n            index = 0\n            for epoch in range(min(number, len(self.sentences))):\n                sent = self.sentences[sorted_pr[index]]\n                sent = normalize_whitespace(sent)\n                top_sentences.append(sent)\n                index += 1\n        if concate :\n            return \",\".join(top_sentences)\n        return top_sentences\n\n    def analyze(self, text, stop_words=None):\n        self.text_str = text\n        self.sentences = sent_tokenize(self.text_str)\n\n        tokenized_sentences = [word_tokenize(sent) for sent in self.sentences]\n\n        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n\n        self.pr_vector = self._run_page_rank(similarity_matrix)\n\n\ndef get_sum_textrank(text, k_rank=5) :\n    tr4sh = TextRank4Sentences()\n    tr4sh.analyze(text)\n    text_str = tr4sh.get_top_sentences(k_rank)\n    return text_str  ","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:00:36.94111Z","iopub.execute_input":"2023-05-23T18:00:36.941423Z","iopub.status.idle":"2023-05-23T18:00:36.97726Z","shell.execute_reply.started":"2023-05-23T18:00:36.941391Z","shell.execute_reply":"2023-05-23T18:00:36.975952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get text rank\nprint(\"calculate_textrank\")\ntext_rank = TextRank4Sentences()\n\ndf_train[\"text_rank\"] = df_train['document'].map(lambda x: get_sum_textrank(x))\ndf_valid[\"text_rank\"] = df_valid['document'].map(lambda x: get_sum_textrank(x))\ndf_test[\"text_rank\"] = df_test['document'].map(lambda x: get_sum_textrank(x))\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:00:36.982956Z","iopub.execute_input":"2023-05-23T18:00:36.984011Z","iopub.status.idle":"2023-05-23T18:00:39.042211Z","shell.execute_reply.started":"2023-05-23T18:00:36.98397Z","shell.execute_reply":"2023-05-23T18:00:39.040879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Concat text_rank and first_output","metadata":{}},{"cell_type":"code","source":"special_token = \"<txtRank>\"\nend_special_token = \"</txtRank>\"\n\ndef concate_tr_fo(data):\n    return data[\"first_output\"] + \" \" + special_token + \" \" + data['text_rank'] + \" \"+ end_special_token\n\nprint('concat text_rank')\ndf_train['second_document'] = df_train.apply(concate_tr_fo, axis=1)\ndf_valid['second_document'] = df_valid.apply(concate_tr_fo, axis=1)\ndf_test['second_document'] = df_test.apply(concate_tr_fo, axis=1)\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:00:39.043832Z","iopub.execute_input":"2023-05-23T18:00:39.044652Z","iopub.status.idle":"2023-05-23T18:00:39.08839Z","shell.execute_reply.started":"2023-05-23T18:00:39.044597Z","shell.execute_reply":"2023-05-23T18:00:39.084052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Second model","metadata":{}},{"cell_type":"code","source":"model_name = \"facebook/bart-base\"\n\nsecond_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# tokenization\nencoder_max_length = 512  # demo\ndecoder_max_length = 64","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:00:39.092881Z","iopub.execute_input":"2023-05-23T18:00:39.095055Z","iopub.status.idle":"2023-05-23T18:00:45.629288Z","shell.execute_reply.started":"2023-05-23T18:00:39.095023Z","shell.execute_reply":"2023-05-23T18:00:45.627704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"extra_token = \"<extra>\"\nend_extra_token = \"</extra>\"\nlist_tokens = [special_token, end_special_token, end_extra_token, extra_token]\n\ntokenizer.add_tokens(list_tokens, special_tokens=True) ##This line is updated\ntokenizer.additional_special_tokens= list_tokens\ntokenizer.additional_special_tokens\nsecond_model.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:00:45.631091Z","iopub.execute_input":"2023-05-23T18:00:45.631755Z","iopub.status.idle":"2023-05-23T18:00:46.199317Z","shell.execute_reply.started":"2023-05-23T18:00:45.63171Z","shell.execute_reply":"2023-05-23T18:00:46.194497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pack data\ndef flatten_2(example):\n    return {\n        \"document\": example[\"second_document\"],\n        \"summary\": example[\"summary\"],\n    }\n\ndf_custom_train_2 = Dataset.from_pandas(df_train)\ndf_custom_valid_2 = Dataset.from_pandas(df_valid)\ndf_custom_test_2 = Dataset.from_pandas(df_test)\n\ndf_custom_train_2 = df_custom_train_2.map(flatten_2)\ndf_custom_valid_2 = df_custom_valid_2.map(flatten_2)\ndf_custom_test_2 = df_custom_test_2.map(flatten_2)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:00:46.201082Z","iopub.execute_input":"2023-05-23T18:00:46.201766Z","iopub.status.idle":"2023-05-23T18:00:46.360175Z","shell.execute_reply.started":"2023-05-23T18:00:46.201719Z","shell.execute_reply":"2023-05-23T18:00:46.358971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_tokenize_preprocess(batch, tokenizer, max_source_length, max_target_length):\n    source, target = batch[\"document\"], batch[\"summary\"]\n    source_tokenized = tokenizer(source, padding=\"max_length\", truncation=True, max_length=max_source_length )\n    target_tokenized = tokenizer(target, padding=\"max_length\", truncation=True, max_length=max_target_length )\n\n    batch = {k: v for k, v in source_tokenized.items()}\n    # Ignore padding in the loss\n    batch[\"labels\"] = [\n        [-100 if token == tokenizer.pad_token_id else token for token in l]\n        for l in target_tokenized[\"input_ids\"]\n    ]\n    return batch\n\n\ntrain_data_2 = df_custom_train_2.map( lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=df_custom_train_2.column_names,\n)\n\nvalidation_data_2 = df_custom_valid_2.map(lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=df_custom_valid_2.column_names,\n)\n\n\ntest_data_2 = df_custom_test_2.map(lambda batch: batch_tokenize_preprocess(\n        batch, tokenizer, encoder_max_length, decoder_max_length\n    ),\n    batched=True,\n    remove_columns=df_custom_test_2.column_names,\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:00:46.362517Z","iopub.execute_input":"2023-05-23T18:00:46.36341Z","iopub.status.idle":"2023-05-23T18:00:46.687982Z","shell.execute_reply.started":"2023-05-23T18:00:46.363365Z","shell.execute_reply":"2023-05-23T18:00:46.686621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"/NLP_project/bart_2\",\n    seed = 42,\n    data_seed = 42,\n    num_train_epochs=4,  # demo\n    do_train=True,\n    do_eval=True,\n    per_device_train_batch_size=4,  # demo\n    per_device_eval_batch_size=4,\n    warmup_steps=500,\n    weight_decay=0.1,\n    label_smoothing_factor=0.1,\n    predict_with_generate=True,\n    logging_dir=\"logs\",\n    logging_steps=6000,\n    evaluation_strategy=\"steps\",\n    save_total_limit = 5,\n    save_strategy = \"steps\",\n    save_steps = 6000,\n    load_best_model_at_end=True,\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=second_model)\n\ntrainer = Seq2SeqTrainer(\n    model=second_model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data_2,\n    eval_dataset=validation_data_2,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\nsecond_model.device","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:00:46.689705Z","iopub.execute_input":"2023-05-23T18:00:46.69069Z","iopub.status.idle":"2023-05-23T18:00:46.867687Z","shell.execute_reply.started":"2023-05-23T18:00:46.690648Z","shell.execute_reply":"2023-05-23T18:00:46.866407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"print(trainer.evaluate())","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:00:46.8731Z","iopub.execute_input":"2023-05-23T18:00:46.876074Z","iopub.status.idle":"2023-05-23T18:00:55.508844Z","shell.execute_reply.started":"2023-05-23T18:00:46.876023Z","shell.execute_reply":"2023-05-23T18:00:55.507609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"start train 2\")\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:00:55.510371Z","iopub.execute_input":"2023-05-23T18:00:55.511169Z","iopub.status.idle":"2023-05-23T18:01:00.818373Z","shell.execute_reply.started":"2023-05-23T18:00:55.511122Z","shell.execute_reply":"2023-05-23T18:01:00.816819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save model","metadata":{}},{"cell_type":"code","source":"print(\"Finish and get output\")\ntrainer.save_model(\"/kaggle/working/NLP_project/best_2_model\")\nprint(trainer.evaluate())","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:01:00.820306Z","iopub.execute_input":"2023-05-23T18:01:00.827031Z","iopub.status.idle":"2023-05-23T18:01:11.430722Z","shell.execute_reply.started":"2023-05-23T18:01:00.826972Z","shell.execute_reply":"2023-05-23T18:01:11.429592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate Test","metadata":{}},{"cell_type":"code","source":"tester = Seq2SeqTrainer(\n    model=second_model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_data_2,\n    eval_dataset=test_data_2,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\ntester.evaluate()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:01:11.435694Z","iopub.execute_input":"2023-05-23T18:01:11.436882Z","iopub.status.idle":"2023-05-23T18:01:20.006738Z","shell.execute_reply.started":"2023-05-23T18:01:11.436835Z","shell.execute_reply":"2023-05-23T18:01:19.999222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Output second model","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# add new cols\ndf_train = gen_output(second_model, df_custom_train_2, df_train, 'second_output')\ndf_valid = gen_output(second_model, df_custom_valid_2, df_valid, 'second_output')\ndf_test = gen_output(second_model, df_custom_test_2, df_test, 'second_output')\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:01:20.011157Z","iopub.execute_input":"2023-05-23T18:01:20.011457Z","iopub.status.idle":"2023-05-23T18:01:35.439539Z","shell.execute_reply.started":"2023-05-23T18:01:20.011427Z","shell.execute_reply":"2023-05-23T18:01:35.438312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save file csv","metadata":{}},{"cell_type":"code","source":"# Save results\nprint(\"Save results\")\ndf_train.to_csv(\"df_train_result_textrank.csv\")\ndf_valid.to_csv(\"df_valid_result_textrank.csv\")\ndf_test.to_csv(\"df_test_result_textrank.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-05-23T18:01:35.44104Z","iopub.execute_input":"2023-05-23T18:01:35.442059Z","iopub.status.idle":"2023-05-23T18:01:35.486485Z","shell.execute_reply.started":"2023-05-23T18:01:35.442016Z","shell.execute_reply":"2023-05-23T18:01:35.48524Z"},"trusted":true},"execution_count":null,"outputs":[]}]}